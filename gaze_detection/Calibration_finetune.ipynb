{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "61a26ae3-9d4f-43cb-8156-fae08550e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from models.gaze_model import FineTuneModel, FaceModel, EyesModel, FaceGridModel\n",
    "from dataset.dataset import GazeDetectionDataset\n",
    "from facemesh import FaceMeshBlock, FaceMesh\n",
    "from pupil_detection import IrisLM, IrisBlock\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e7b01f69-f2cd-47b7-8219-0b927f8a7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: FineTuneModel, dataloader_train: DataLoader):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for i, data in enumerate(dataloader_train):\n",
    "        inputs, labels, inputs_eye_l, inputs_eye_r, inputs_mask = data['image'], data['coordinates'], \\\n",
    "                                                     data['eye_l'], data['eye_r'], data['face_mask']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, inputs_eye_l, inputs_eye_r, inputs_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        preds_list += outputs.cpu().detach().tolist()\n",
    "        labels_list += labels.cpu().detach().tolist()\n",
    "        total_loss += loss.cpu().item()\n",
    "\n",
    "    loss = total_loss / (i + 1)\n",
    "    mape_value = mape(labels_list, preds_list)\n",
    "    return loss, mape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b48c06d3-2104-4e47-aed9-411d3604fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model: FineTuneModel, dataloader_val: DataLoader):\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for i, data in enumerate(dataloader_val):\n",
    "        inputs, labels, inputs_eye_l, inputs_eye_r, inputs_mask = data['image'], data['coordinates'], \\\n",
    "                                                     data['eye_l'], data['eye_r'], data['face_mask']\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, inputs_eye_l, inputs_eye_r, inputs_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.cpu().item()\n",
    "        preds_list += outputs.cpu().detach().tolist()\n",
    "        labels_list += labels.cpu().detach().tolist()\n",
    "        \n",
    "    loss = total_loss / (i + 1)\n",
    "    mape_value = mape(labels_list, preds_list)\n",
    "    return loss, mape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "16304642-7f29-4ea3-a790-17abbf719a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bc24b96c-f1e6-4c4a-9a39-3be67f53f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4c8ef952-95b2-4088-af7f-40ec39166156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 108.65it/s]\n"
     ]
    }
   ],
   "source": [
    "base_folder = \"./real_experiment/calibration_dataset/\"\n",
    "frames_folder = \"frames\"\n",
    "annotations_folder = \"annotations\"\n",
    "frames_folders_path = os.path.join(base_folder, frames_folder)\n",
    "dfs = []\n",
    "for frames_name in tqdm(os.listdir(frames_folders_path)):\n",
    "    ann_path = os.path.join(base_folder, annotations_folder, f\"{frames_name}.txt\").replace(\"frames\", \"points\")\n",
    "    frames_path = os.path.join(frames_folders_path, frames_name)\n",
    "    p = Path(frames_path).glob('*.png')\n",
    "    paths = [str(path.absolute()) for path in p]\n",
    "    df_files = pd.DataFrame({\"paths\": paths})\n",
    "    df_files[\"ind\"] = df_files.paths.apply(lambda x: Path(x).stem)\n",
    "    df = pd.read_csv(\n",
    "        ann_path,\n",
    "        sep = \" \",\n",
    "        header=None\n",
    "    )\n",
    "    cols = [\n",
    "        \"timestamp\", \"x_gt\", \"y_gt\", \"x1\", \"y1\",\n",
    "        \"x2\", \"y2\", \"screen_w\", \"screen_h\"\n",
    "    ]\n",
    "    df.columns = cols\n",
    "    \n",
    "    df[\"x_normalized\"] = df[\"x_gt\"] / df[\"screen_w\"]\n",
    "    df[\"y_normalized\"] = df[\"y_gt\"] / df[\"screen_h\"]\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].apply(str)\n",
    "    full_df = df_files.merge(df, left_on=\"ind\", right_on=\"timestamp\").drop(columns = [\"ind\"])\n",
    "    dfs.append(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "947fa7b0-2fe6-4867-8f4e-4cbe03ee46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_folder = \"./real_experiment/calibration_dataset/frames/\"\n",
    "# p = Path(frames_folder).glob('*.png')\n",
    "# paths = [str(path.absolute()) for path in p]\n",
    "# df_files = pd.DataFrame({\"paths\": paths})\n",
    "# df_files[\"ind\"] = df_files.paths.apply(lambda x: Path(x).stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b34b0382-555f-41af-90bf-eaca997e6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\n",
    "#     \"./real_experiment/points_train.txt\",\n",
    "#     sep = \" \",\n",
    "#     header=None\n",
    "# )\n",
    "# cols = [\n",
    "#     \"timestamp\", \"x_gt\", \"y_gt\", \"x1\", \"y1\",\n",
    "#     \"x2\", \"y2\", \"screen_w\", \"screen_h\"\n",
    "# ]\n",
    "# df.columns = cols\n",
    "\n",
    "# df[\"x_normalized\"] = df[\"x_gt\"] / df[\"screen_w\"]\n",
    "# df[\"y_normalized\"] = df[\"y_gt\"] / df[\"screen_h\"]\n",
    "# df[\"timestamp\"] = df[\"timestamp\"].apply(str)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "693c096c-6a6a-43b7-a70a-2fdc9ad07f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full_df = df_files.merge(df, left_on=\"ind\", right_on=\"timestamp\").drop(columns = [\"ind\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "56ede81a-ad9c-40bc-a723-fd2600603f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "201c087c-ddbc-44d3-b2e1-e757ddb40373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 12)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2097dc24-d0c2-49bd-9aa7-b9828ebe475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"participant_name\"] = full_df[\"paths\"].apply(lambda x: x.split(\"/\")[-2].split(\"_\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2020d762-b04d-4219-a57d-c8248354fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_train = full_df[full_df[\"participant_name\"] != \"marina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "31745b59-635d-4207-bccd-29a988b1016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diff_person = full_df[full_df[\"participant_name\"] == \"marina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4b36d80d-17af-4490-8d99-9883292febfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diff_person, val_df_hard = train_test_split(test_diff_person, test_size = 0.3, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e81f0df4-0eaa-4348-b249-80aa7b9fbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_train = pd.read_csv(\"real_experiment/calibration_dataset/cleaned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "308fb5b4-7cbc-4d7d-9cf3-ee9050e22679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>screen_w</th>\n",
       "      <th>screen_h</th>\n",
       "      <th>participant_name</th>\n",
       "      <th>...</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x_normalized</th>\n",
       "      <th>y_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/projects/tweakle/gaze_detection/r...</td>\n",
       "      <td>1.697561e+09</td>\n",
       "      <td>2474</td>\n",
       "      <td>1520</td>\n",
       "      <td>misha</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.666667</td>\n",
       "      <td>1010.333333</td>\n",
       "      <td>1205.666667</td>\n",
       "      <td>0.376044</td>\n",
       "      <td>0.740570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/projects/tweakle/gaze_detection/r...</td>\n",
       "      <td>1.697561e+09</td>\n",
       "      <td>2474</td>\n",
       "      <td>1520</td>\n",
       "      <td>misha</td>\n",
       "      <td>...</td>\n",
       "      <td>1342.500000</td>\n",
       "      <td>2451.500000</td>\n",
       "      <td>1497.000000</td>\n",
       "      <td>0.960287</td>\n",
       "      <td>0.935855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/projects/tweakle/gaze_detection/r...</td>\n",
       "      <td>1.697561e+09</td>\n",
       "      <td>2474</td>\n",
       "      <td>1520</td>\n",
       "      <td>misha</td>\n",
       "      <td>...</td>\n",
       "      <td>386.333333</td>\n",
       "      <td>978.666667</td>\n",
       "      <td>546.333333</td>\n",
       "      <td>0.363244</td>\n",
       "      <td>0.306798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/projects/tweakle/gaze_detection/r...</td>\n",
       "      <td>1.697561e+09</td>\n",
       "      <td>2474</td>\n",
       "      <td>1520</td>\n",
       "      <td>misha</td>\n",
       "      <td>...</td>\n",
       "      <td>1125.000000</td>\n",
       "      <td>790.500000</td>\n",
       "      <td>1285.000000</td>\n",
       "      <td>0.287187</td>\n",
       "      <td>0.792763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/projects/tweakle/gaze_detection/r...</td>\n",
       "      <td>1.697561e+09</td>\n",
       "      <td>2474</td>\n",
       "      <td>1520</td>\n",
       "      <td>misha</td>\n",
       "      <td>...</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>2221.666667</td>\n",
       "      <td>1069.000000</td>\n",
       "      <td>0.865670</td>\n",
       "      <td>0.650658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paths     timestamp  screen_w  \\\n",
       "0  /home/ubuntu/projects/tweakle/gaze_detection/r...  1.697561e+09      2474   \n",
       "1  /home/ubuntu/projects/tweakle/gaze_detection/r...  1.697561e+09      2474   \n",
       "2  /home/ubuntu/projects/tweakle/gaze_detection/r...  1.697561e+09      2474   \n",
       "3  /home/ubuntu/projects/tweakle/gaze_detection/r...  1.697561e+09      2474   \n",
       "4  /home/ubuntu/projects/tweakle/gaze_detection/r...  1.697561e+09      2474   \n",
       "\n",
       "   screen_h participant_name  ...           y1           x2           y2  \\\n",
       "0      1520            misha  ...  1045.666667  1010.333333  1205.666667   \n",
       "1      1520            misha  ...  1342.500000  2451.500000  1497.000000   \n",
       "2      1520            misha  ...   386.333333   978.666667   546.333333   \n",
       "3      1520            misha  ...  1125.000000   790.500000  1285.000000   \n",
       "4      1520            misha  ...   909.000000  2221.666667  1069.000000   \n",
       "\n",
       "   x_normalized  y_normalized  \n",
       "0      0.376044      0.740570  \n",
       "1      0.960287      0.935855  \n",
       "2      0.363244      0.306798  \n",
       "3      0.287187      0.792763  \n",
       "4      0.865670      0.650658  \n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "20bbcae5-5d6b-40b7-b265-e3afc39fffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = None\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 1e-4\n",
    "REDUCE_FACTOR = 0.5\n",
    "PATIENCE = 3\n",
    "NUM_EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CHECKPOINTS_PATH = \"./checkpoints/\"\n",
    "EXPERIMENT_NAME = \"calibration_unfreezed_guys_cleaned_weighted_loss_more_warmup_aug_tune_tune_tune\"\n",
    "LOSS_WEIGHTS = [0.3, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "610c2d60-22c9-4d90-a82e-7ac503a43cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(full_df_train.head(NUM_SAMPLES), test_size = 0.1, random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(full_df_train, test_size = 0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1d952650-2c8a-4337-a684-46acd63c67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = A.Compose(\n",
    "    [\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Blur(p=0.3),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.RandomGamma(p=0.5),\n",
    "        A.ImageCompression(quality_lower=75, p=0.5),\n",
    "        A.MotionBlur(p=0.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4b5aaf43-2d2f-47eb-939c-db3a5357e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list = [A.Resize(192, 192)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2e475e79-a8ec-46a2-8ab1-18348c4afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n",
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "trans_list = [A.Resize(192, 192)]\n",
    "dataset_train = GazeDetectionDataset(data = train_df, transform_list=trans_list,\n",
    "                                     to_tensors=True, device=device, screen_features=False,\n",
    "                                     transform=augmentations, augmentation_factor = 7)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=0)\n",
    "dataset_val = GazeDetectionDataset(data = val_df, transform_list=trans_list,\n",
    "                                   to_tensors=True, device=device, screen_features=False)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)\n",
    "dataset_val_hard = GazeDetectionDataset(data = val_df_hard, transform_list=trans_list,\n",
    "                                   to_tensors=True, device=device, screen_features=False)\n",
    "dataloader_val_hard = DataLoader(dataset_val_hard, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "48e5e798-e956-4619-9ec6-314e5a6353df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, device = \"cuda:0\", eps=1e-16, weights = None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weights = weights\n",
    "        self.device = device\n",
    "\n",
    "    def __mse_loss(self, input, target, weights):\n",
    "        if weights is not None:\n",
    "            weights_tensor = torch.from_numpy(np.array(weights)).to(device)\n",
    "            return torch.sum(weights_tensor * (input - target) ** 2) / weights_tensor.sum()\n",
    "        return torch.mean((input - target) ** 2)\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.__mse_loss(yhat, y, self.weights) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d092dfd9-eec6-45e9-b8d1-3ddf8e8df61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyesModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_eyes: nn.Module):\n",
    "        super(EyesModel, self).__init__()\n",
    "        self.backbone = pretrained_model_eyes.backbone\n",
    "        self.regression_head_eyes = nn.Sequential(\n",
    "            IrisBlock(128, 128), IrisBlock(128, 128),\n",
    "            IrisBlock(128, 128, stride=2),\n",
    "            IrisBlock(128, 128), IrisBlock(128, 128),\n",
    "            IrisBlock(128, 128, stride=2),\n",
    "            IrisBlock(128, 128), IrisBlock(128, 128),\n",
    "        )\n",
    "        # connect eyes\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * 128 * 1 * 1, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_eye_l, x_eye_r):\n",
    "        x_eye_l = self.backbone(x_eye_l)\n",
    "        x_eye_l = self.regression_head_eyes(x_eye_l)\n",
    "        x_eye_l = x_eye_l.view(-1, 128 * 1 * 1)\n",
    "\n",
    "        x_eye_r = self.backbone(x_eye_r)\n",
    "        x_eye_r = self.regression_head_eyes(x_eye_r)\n",
    "        x_eye_r = x_eye_r.view(-1, 128 * 1 * 1)\n",
    "        x = torch.cat([x_eye_l, x_eye_r], 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e14d48c0-7fc2-4599-b458-a0f0d917a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_face: nn.Module):\n",
    "        super(FaceModel, self).__init__()\n",
    "        self.backbone = pretrained_model_face.backbone\n",
    "        self.regression_head_face = nn.Sequential(\n",
    "            FaceMeshBlock(128, 128, stride=2),\n",
    "            FaceMeshBlock(128, 128),\n",
    "            FaceMeshBlock(128, 128),\n",
    "            # FaceMeshBlock(128, 128),\n",
    "            # FaceMeshBlock(128, 128),\n",
    "            nn.Conv2d(128, 32, 1),\n",
    "            nn.PReLU(32),\n",
    "            FaceMeshBlock(32, 32),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 3 * 3, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, x):\n",
    "        return x.to(torch.float32) * 2.0 - 1.0\n",
    "\n",
    "    def forward(self, x_face):\n",
    "        x_face = self._preprocess(x_face)\n",
    "        x_face = nn.ReflectionPad2d((1, 0, 1, 0))(x_face)\n",
    "        x_face = self.backbone(x_face)\n",
    "        x_face = self.regression_head_face(x_face)\n",
    "        x_face = x_face.view(-1, 32 * 3 * 3)\n",
    "        x = self.fc(x_face)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2d5191ec-6827-4e7c-b225-8ffe7816b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_face: nn.Module,\n",
    "        pretrained_model_eyes: nn.Module,\n",
    "        screen_features: bool = False,\n",
    "    ):\n",
    "        super(FineTuneModel, self).__init__()\n",
    "        self.face_model = FaceModel(pretrained_model_face)\n",
    "        self.eyes_model = EyesModel(pretrained_model_eyes)\n",
    "        self.face_grid_model = FaceGridModel()\n",
    "        self.screen_features = screen_features\n",
    "        if not screen_features:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(128+64+128, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(32, 2),\n",
    "            )\n",
    "        else:\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(128+64+128, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(128, 13),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            self.layer_norm = nn.LayerNorm(16)\n",
    "            self.fc2 = nn.Linear(16, 2)\n",
    "            \n",
    "\n",
    "    def _preprocess(self, x):\n",
    "        return x.to(torch.float32) * 2.0 - 1.0\n",
    "        \n",
    "    def forward(self, x_face, x_eye_l, x_eye_r, x_grid, x_screen = None):\n",
    "        if self.screen_features and x_screen is None:\n",
    "            raise Exception(\"You should pass screen features\")\n",
    "        if not self.screen_features and x_screen is not None:\n",
    "            warnings.warn(\"Screen fearures won't be used\")\n",
    "        x_eyes = self.eyes_model(x_eye_l, x_eye_r)\n",
    "        x_face = self.face_model(x_face)\n",
    "        x_grid = self.face_grid_model(x_grid)\n",
    "        x = torch.cat([x_eyes, x_face, x_grid], axis = 1)\n",
    "        if not self.screen_features:\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "            x = torch.cat([x, x_screen], axis = 1)\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "15072f79-50b6-4ac9-baa0-a1f097d6ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/kmisterios/tweakle-gaze-calibration/145784d0d37b474d92028585d653755f\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     best_model_epoch [21] : (0, 3)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [21]            : (0, 20)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr [21]               : (1.2537500000000002e-05, 5.000000000000001e-05)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse_train [21]       : (2.130670816312247, 2.8134667618515072)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse_val [21]         : (0.6479468624619621, 0.7003415674279497)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse_val_hard [21]    : (0.7605066532391761, 0.844291527853334)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_mape [21]       : (0.24925809430003465, 0.31600344528933216)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_mape [21]         : (0.4565612808024509, 0.503993925711079)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_mape_hard [21]    : (0.4203563317669311, 0.5008240708979279)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : calibration_unfreezed_guys_cleaned_weighted_loss_more_warmup_aug_tune_tune\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (212.16 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/kmisterios/tweakle-gaze-calibration/d7552d128f364dfdaa2bf55acfcca249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "experiment = Experiment(\n",
    "  api_key=\"4qtNKAjcucKnOrwC4pRvPaHRv\",\n",
    "  project_name=\"tweakle-gaze-calibration\",\n",
    "  workspace=\"kmisterios\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e93f29e9-d73e-4dab-be77-04caf5c4024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name(f\"{EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bc58acae-c049-46e0-9289-f60f9e82aaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_face = FaceMesh()\n",
    "pretrained_model_face.load_weights(\"./weights/facemesh.pth\")\n",
    "\n",
    "model_path = \"./weights/irislandmarks.pth\"\n",
    "pretrained_model_eyes = IrisLM()\n",
    "weights = torch.load(model_path)\n",
    "pretrained_model_eyes.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "31136ae3-a078-4b14-853c-4c4c5b3352db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_PATH = \"./checkpoints\"\n",
    "# EXPERIMENT_NAME_ORIG = \"face_eyes_mask_more_layers_more_patience_weighted_loss_more_complexity_tune\"\n",
    "EXPERIMENT_NAME_ORIG = \"calibration_unfreezed_guys_cleaned_weighted_loss_more_warmup_aug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bc4a1776-3ab2-411b-ba39-880dae6f80f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneModel(\n",
       "  (face_model): FaceModel(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): PReLU(num_parameters=16)\n",
       "      (2): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=16)\n",
       "      )\n",
       "      (3): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "          (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=16)\n",
       "      )\n",
       "      (4): FaceMeshBlock(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), groups=16)\n",
       "          (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=32)\n",
       "      )\n",
       "      (5): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=32)\n",
       "      )\n",
       "      (6): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=32)\n",
       "      )\n",
       "      (7): FaceMeshBlock(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), groups=32)\n",
       "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (8): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (9): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (10): FaceMeshBlock(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (11): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (12): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (13): FaceMeshBlock(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (14): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (15): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "    )\n",
       "    (regression_head_face): Sequential(\n",
       "      (0): FaceMeshBlock(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (1): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (2): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (3): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): PReLU(num_parameters=32)\n",
       "      (5): FaceMeshBlock(\n",
       "        (convs): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (act): PReLU(num_parameters=32)\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=288, out_features=128, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (eyes_model): EyesModel(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): PReLU(num_parameters=64)\n",
       "      (2): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=32)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (3): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=32)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (4): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=32)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (5): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=32)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (6): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (7): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (8): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (9): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (10): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (11): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "    )\n",
       "    (regression_head_eyes): Sequential(\n",
       "      (0): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (1): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (2): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (3): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (4): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (5): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (6): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "      (7): IrisBlock(\n",
       "        (conv_prelu): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (depthwiseconv_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (prelu): PReLU(num_parameters=128)\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (face_grid_model): FaceGridModel(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=625, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FineTuneModel(pretrained_model_face, pretrained_model_eyes, screen_features=False).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINTS_PATH, f\"best_{EXPERIMENT_NAME_ORIG}.pt\")))\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "82204885-cad1-4e53-89cf-c14b5db9e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3ec0a8f8-18fa-4228-ade8-1a4db13acfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss(weights = LOSS_WEIGHTS)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor= REDUCE_FACTOR, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "fe079090-d10b-45a9-946a-ff663e0345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.001,\n",
    "    end_factor=1.0,\n",
    "    total_iters=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedb39f-2aa0-4c48-908b-2f2c46fb3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0: Train loss: 2.715; Val loss: 0.677;; Val loss hard: 0.844;\n",
      "Best model saved on epoch 0\n",
      "\n",
      "Epoch: 1: Train loss: 2.628; Val loss: 0.676;; Val loss hard: 0.773;\n",
      "Best model saved on epoch 1\n",
      "\n",
      "Epoch: 2: Train loss: 2.562; Val loss: 0.701;; Val loss hard: 0.771;\n",
      "Best model saved on epoch 2\n",
      "\n",
      "Epoch: 3: Train loss: 2.493; Val loss: 0.685;; Val loss hard: 0.799;\n",
      "Best model saved on epoch 2\n",
      "\n",
      "Epoch: 4: Train loss: 2.457; Val loss: 0.662;; Val loss hard: 0.831;\n",
      "Best model saved on epoch 2\n",
      "\n",
      "Epoch: 5: Train loss: 2.352; Val loss: 0.662;; Val loss hard: 0.794;\n",
      "Best model saved on epoch 2\n",
      "\n",
      "Epoch: 6: Train loss: 2.261; Val loss: 0.659;; Val loss hard: 0.8;\n",
      "Best model saved on epoch 2\n",
      "\n",
      "Epoch: 7: Train loss: 2.189; Val loss: 0.659;; Val loss hard: 0.808;\n",
      "Best model saved on epoch 2\n"
     ]
    }
   ],
   "source": [
    "val_loss_min = np.inf\n",
    "epoch_save = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_mape = train(model, dataloader_train)\n",
    "    val_loss, val_mape = eval(model, dataloader_val)\n",
    "    val_loss_hard, val_mape_hard = eval(model, dataloader_val_hard)\n",
    "    \n",
    "    if epoch <= 4:\n",
    "        warmup.step()\n",
    "        current_lr = warmup.optimizer.param_groups[0]['lr']\n",
    "    else:\n",
    "        scheduler.step(val_loss_hard)\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    if val_loss_min > val_loss_hard:\n",
    "        val_loss_min = val_loss_hard\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINTS_PATH, f\"best_{EXPERIMENT_NAME}.pt\"))\n",
    "        epoch_save = epoch\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch: {epoch}: Train loss: {round(train_loss, 3)}; Val loss: {round(val_loss, 3)};; Val loss hard: {round(val_loss_hard, 3)};')\n",
    "    if epoch_save is not None:\n",
    "        print(f'Best model saved on epoch {epoch_save}')\n",
    "    experiment.log_metrics({\n",
    "        \"rmse_val\": val_loss,\n",
    "        \"rmse_val_hard\": val_loss_hard,\n",
    "        \"rmse_train\": train_loss,\n",
    "        \"best_model_epoch\": epoch_save,\n",
    "        \"train_mape\": train_mape,\n",
    "        \"val_mape\": val_mape,\n",
    "        \"val_mape_hard\": val_mape_hard,\n",
    "        \"epoch\": epoch,\n",
    "        \"lr\": current_lr\n",
    "    })\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afb154-f84e-4517-aa7b-3946ebfb7588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450b74c-504d-4825-89f9-e47fa3bf35dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FineTuneModel(pretrained_model_face, pretrained_model_eyes, screen_features=False).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINTS_PATH, f\"best_{EXPERIMENT_NAME}.pt\")))\n",
    "# model.load_state_dict(torch.load(os.path.join(CHECKPOINTS_PATH, f\"best_raw_images.pt\")))\n",
    "criterion = RMSELoss(weights = LOSS_WEIGHTS)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559dcee-a5be-474a-8543-ce20c3c513bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca87a9-1468-4cce-9c83-2805e7a16e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = GazeDetectionDataset(data = test_df, transform_list=trans_list, to_tensors=True, device=device, screen_features=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffbc4a-fb83-4675-97b8-35c3d6a18d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "labels_list = []\n",
    "losses = 0\n",
    "for i, data in tqdm(enumerate(dataloader_test), total = len(dataloader_test)):\n",
    "    inputs, labels, inputs_eye_l, inputs_eye_r, inputs_mask = data['image'], data['coordinates'], \\\n",
    "                                                     data['eye_l'], data['eye_r'], data['face_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, inputs_eye_l, inputs_eye_r, inputs_mask)\n",
    "    loss = criterion(outputs, labels)\n",
    "    losses += loss.detach().cpu().item()\n",
    "    pred = outputs.cpu().numpy()\n",
    "    preds.append(pred)\n",
    "    labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test loss: {round(losses / (i + 1), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36c11b-9752-4f41-bd87-7e6e025910f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test loss: {round(losses / (i + 1), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8bab1-7d48-407b-b8ae-33667f920247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = np.vstack(preds)\n",
    "labels = np.vstack(labels_list)\n",
    "\n",
    "mape_value = mape(labels, preds)\n",
    "print(f\"Test MAPE: {mape_value}\")\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "\n",
    "test_df_copy[\"pred_x\"] = preds.T[0]\n",
    "test_df_copy[\"pred_y\"] = preds.T[1]\n",
    "\n",
    "test_df_copy[['x_normalized', 'y_normalized', 'pred_x', 'pred_y']].tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145ba03-1984-43a8-807d-a99576dbf2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3bc7a-543e-475c-83e1-5b28424cb588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a244b-db91-4f86-b15d-38b28a8e61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = GazeDetectionDataset(data = test_diff_person, transform_list=trans_list, to_tensors=True, device=device, screen_features=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a825a89-72eb-4698-95ea-465dfb9a38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "labels_list = []\n",
    "losses = 0\n",
    "for i, data in tqdm(enumerate(dataloader_test), total = len(dataloader_test)):\n",
    "    inputs, labels, inputs_eye_l, inputs_eye_r, inputs_mask = data['image'], data['coordinates'], \\\n",
    "                                                     data['eye_l'], data['eye_r'], data['face_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, inputs_eye_l, inputs_eye_r, inputs_mask)\n",
    "    loss = criterion(outputs, labels)\n",
    "    losses += loss.detach().cpu().item()\n",
    "    pred = outputs.cpu().numpy()\n",
    "    preds.append(pred)\n",
    "    labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test loss: {round(losses / (i + 1), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da2836d-1fff-4319-be90-42470f8bcf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = np.vstack(preds)\n",
    "labels = np.vstack(labels_list)\n",
    "\n",
    "mape_value = mape(labels, preds)\n",
    "print(f\"Test MAPE: {mape_value}\")\n",
    "\n",
    "test_df_copy = test_diff_person.copy()\n",
    "\n",
    "test_df_copy[\"pred_x\"] = preds.T[0]\n",
    "test_df_copy[\"pred_y\"] = preds.T[1]\n",
    "\n",
    "test_df_copy[['x_normalized', 'y_normalized', 'pred_x', 'pred_y']].tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b1d25-72c1-4a84-a431-6386d3ed48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_copy.to_csv(\"./real_experiment/res_calibration_unfreezed_guys_cleaned_weighted_loss_more_warmup_aug_tune_tune_tune.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cd6f6-24ca-403e-b0fd-174fd402bebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGeneral",
   "language": "python",
   "name": "envgeneral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
